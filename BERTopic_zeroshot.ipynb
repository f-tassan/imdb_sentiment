{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Attempting to create categories using Zero-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "# import transforms\n",
    "# from transforms.api import Input, Output, transform\n",
    "from bertopic import BERTopic\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "import ast\n",
    "# from pyspark.sql.types import StructType, StructField, IntegerType, ArrayType, StringType, DoubleType, FloatType\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, PartOfSpeech\n",
    "import spacy\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "# import en_core_web_sm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# from transforms.external.systems import use_external_systems, EgressPolicy, Credential, ExportControl\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic.representation import ZeroShotClassification\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentimentGroup(compound):\n",
    "    if 0.66 <= compound < 1:\n",
    "        return 'Very Good'\n",
    "    elif 0.33 <= compound < 0.66:\n",
    "        return 'Good'\n",
    "    elif 0 <= compound < 0.33:\n",
    "        return 'Neutral Postive'\n",
    "    elif -0.33 <= compound < 0:\n",
    "        return 'Neutral Negative'\n",
    "    elif -0.66 <= compound < -0.33:\n",
    "        return 'Bad'\n",
    "    elif -1 <= compound < -0.66:\n",
    "        return 'Very Bad'\n",
    "       \n",
    " \n",
    "def getSentimentDict(sentence):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    score = analyzer.polarity_scores(sentence)\n",
    "    compound = score #score['compound']\n",
    "    return compound\n",
    "\n",
    "\n",
    "# params\n",
    "# params = params.dataframe()\n",
    "# params = params.collect()[0][:-1]\n",
    "# [n_neighbors, _, min_cluster_size, _, _] = params\n",
    "# n_neighbors = 15\n",
    "# min_cluster_size = 15\n",
    "\n",
    "n_neighbors=15\n",
    "min_topic_size=15\n",
    "min_cluster_size=15\n",
    "top_n_words=10\n",
    "diversity = 0.9\n",
    "\n",
    "# umap model\n",
    "umap_model = UMAP(n_neighbors=n_neighbors, n_components=5, min_dist=0, metric='cosine', random_state=42)\n",
    "\n",
    "# clustering\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\", min_df=2, ngram_range=(1, 2))\n",
    "# representation model\n",
    "keybert_model = KeyBERTInspired()\n",
    "mmr_model = MaximalMarginalRelevance(diversity)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\") #inshallah\n",
    "pos = PartOfSpeech(top_n_words=top_n_words)\n",
    "\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "# All representation models\n",
    "representation_model = {\n",
    "    \"KeyBERT\": keybert_model,\n",
    "    \"MMR\": mmr_model,\n",
    "    \"pos\": pos\n",
    "}\n",
    "\n",
    "#Train the model\n",
    "# model = BERTopic(\n",
    "#   # Pipeline models\n",
    "#   embedding_model = embedding_model,\n",
    "#   umap_model=umap_model,\n",
    "#   hdbscan_model=hdbscan_model,\n",
    "#   vectorizer_model=vectorizer_model,\n",
    "#   representation_model=representation_model,\n",
    "#   # Hyperparameters\n",
    "#   top_n_words = top_n_words,\n",
    "#   min_topic_size = min_topic_size,\n",
    "#   #nr_topics=75,\n",
    "#   ctfidf_model=ctfidf_model,\n",
    "#   verbose=True\n",
    "# )\n",
    "\n",
    "\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2',device=\"cuda\")\n",
    "\n",
    "# Zero-shot\n",
    "# candidate_topics = [\"hotel\", \"staff\", \"location\"]\n",
    "\n",
    "# representation_model = ZeroShotClassification(candidate_topics, model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "model = BERTopic(\n",
    "    verbose=True,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    representation_model= representation_model,\n",
    "    embedding_model = embedding_model\n",
    ")\n",
    "\n",
    "\n",
    "# source_df = source_df.dataframe()\n",
    "# # source_df = source_df.limit(2000)\n",
    "# df = source_df.toPandas()\n",
    "\n",
    "df = pd.read_csv('input/sentence_embedded.csv') # change the filename to your .csv file name and location\n",
    "\n",
    "df = df.dropna(subset=['embedding'])\n",
    "\n",
    "df['embedding'] = df['embedding'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "docs = df.text.to_list()\n",
    "embeddings = np.array(df['embedding'].apply(lambda x: np.array(x).astype(np.float32)).to_list())\n",
    "# embeddings = np.array(df['embedding'].apply(lambda x: np.array(x, dtype=np.float32)).to_list())\n",
    "topics, probs = model.fit_transform(docs, embeddings)\n",
    "df[\"topic\"] = topics\n",
    "df[\"probability\"] = probs\n",
    "\n",
    "# get sentiment\n",
    "df['sentiment_analysis'] = df['text'].apply(lambda x: getSentimentDict(x))\n",
    "df['sentiment_compound'] = df['sentiment_analysis'].apply(lambda x: x['compound'])\n",
    "df['sentiment'] = df['sentiment_compound'].apply(lambda x: getSentimentGroup(x)) # Good Bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL: facebook/bart-large-mnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('temp_files/hier_topics.csv') # change the filename to your .csv file name and location\n",
    "\n",
    "# change these categories to the desired categories\n",
    "candidate_topics = [\"Parking and Transportation\",\n",
    "                    \"Special Occasions and Events\",\n",
    "                    \"Value and Price\",\n",
    "                    \"Room and Ameneties\",\n",
    "                    \"Cleanliness and Maintenance\",\n",
    "                    \"Facilities and Activities\",\n",
    "                    \"Technologhy and Internet\",\n",
    "                    \"Location\",\n",
    "                    \"Service and Staff\",\n",
    "                    \"Food and Beverage\"]\n",
    "\n",
    "zero_shot_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "def classify_document(text):\n",
    "    result = zero_shot_classifier(text, candidate_topics)\n",
    "    return result['labels'][0]  # returns the top category\n",
    "\n",
    "df['Child_Left_Category'] = df['Child_Left_Name'].apply(classify_document)  # replace 'Parent_Name' with the correct column name\n",
    "df['Child_Right_Category'] = df['Child_Right_Name'].apply(classify_document)  # replace 'Parent_Name' with the correct column name\n",
    "\n",
    "df.to_csv(\"temp_files/clusters.csv\", index=False)\n",
    "\n",
    "print(\"Updated CSV file saved to temp_files/clusters.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL: tasksource/deberta-small-long-nli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('temp_files/hier_topics.csv') # change the filename to your .csv file name and location\n",
    "\n",
    "# change these categories to the desired categories\n",
    "candidate_topics = [\"Parking and Transportation\",\n",
    "                    \"Special Occasions and Events\",\n",
    "                    \"Value and Price\",\n",
    "                    \"Room and Ameneties\",\n",
    "                    \"Cleanliness and Maintenance\",\n",
    "                    \"Facilities and Activities\",\n",
    "                    \"Technologhy and Internet\",\n",
    "                    \"Location\",\n",
    "                    \"Service and Staff\",\n",
    "                    \"Food and Beverage\"]\n",
    "\n",
    "zero_shot_classifier = pipeline(\"zero-shot-classification\", model=\"tasksource/deberta-small-long-nli\")\n",
    "\n",
    "def classify_document(text):\n",
    "    result = zero_shot_classifier(text, candidate_topics)\n",
    "    return result['labels'][0]  # returns the top category\n",
    "\n",
    "df['Child_Left_Category'] = df['Child_Left_Name'].apply(classify_document)  # replace 'Parent_Name' with the correct column name\n",
    "df['Child_Right_Category'] = df['Child_Right_Name'].apply(classify_document)  # replace 'Parent_Name' with the correct column name\n",
    "\n",
    "df.to_csv(\"temp_files/clusters2.csv\", index=False)\n",
    "\n",
    "print(\"Updated CSV file saved to temp_files/clusters2.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
