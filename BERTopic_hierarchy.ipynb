{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'NLTK_IMPORT_ERROR' from 'transformers.utils.import_utils' (c:\\Users\\admin\\OneDrive - NEOM\\Desktop\\imdb_sentiment\\.conda\\lib\\site-packages\\transformers\\utils\\import_utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# import libraries\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# import transforms\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# from transforms.api import Input, Output, transform\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbertopic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BERTopic\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UMAP\n",
      "File \u001b[1;32mc:\\Users\\admin\\OneDrive - NEOM\\Desktop\\imdb_sentiment\\.conda\\lib\\site-packages\\bertopic\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbertopic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bertopic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BERTopic\n\u001b[0;32m      3\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.16.2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBERTopic\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\admin\\OneDrive - NEOM\\Desktop\\imdb_sentiment\\.conda\\lib\\site-packages\\bertopic\\_bertopic.py:48\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbertopic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plotting\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbertopic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseCluster\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbertopic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseEmbedder\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbertopic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrepresentation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_mmr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mmr\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbertopic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m select_backend\n",
      "File \u001b[1;32mc:\\Users\\admin\\OneDrive - NEOM\\Desktop\\imdb_sentiment\\.conda\\lib\\site-packages\\bertopic\\backend\\__init__.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Multimodal Embeddings\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbertopic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_multimodal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiModalBackend\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install bertopic[vision]` \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\admin\\OneDrive - NEOM\\Desktop\\imdb_sentiment\\.conda\\lib\\site-packages\\bertopic\\backend\\_multimodal.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Union\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbertopic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseEmbedder\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMultiModalBackend\u001b[39;00m(BaseEmbedder):\n",
      "File \u001b[1;32mc:\\Users\\admin\\OneDrive - NEOM\\Desktop\\imdb_sentiment\\.conda\\lib\\site-packages\\sentence_transformers\\__init__.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCrossEncoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLoggingHandler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoggingHandler\n",
      "File \u001b[1;32mc:\\Users\\admin\\OneDrive - NEOM\\Desktop\\imdb_sentiment\\.conda\\lib\\site-packages\\sentence_transformers\\cross_encoder\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCrossEncoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[0;32m      3\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrossEncoder\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\admin\\OneDrive - NEOM\\Desktop\\imdb_sentiment\\.conda\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentenceEvaluator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceEvaluator\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InputExample\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentenceTransformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fullname, get_device_name, import_from_string\n\u001b[0;32m     21\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\admin\\OneDrive - NEOM\\Desktop\\imdb_sentiment\\.conda\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:27\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautonotebook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m trange\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_torch_npu_available\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_card\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformerModelCardData, generate_model_card\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimilarity_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimilarityFunction\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __MODEL_HUB_ORGANIZATION__, __version__\n",
      "File \u001b[1;32mc:\\Users\\admin\\OneDrive - NEOM\\Desktop\\imdb_sentiment\\.conda\\lib\\site-packages\\sentence_transformers\\model_card.py:28\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer_callback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainerControl, TrainerState\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m sentence_transformers_version\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Transformer\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining_args\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformerTrainingArguments\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fullname, is_accelerate_available, is_datasets_available\n",
      "File \u001b[1;32mc:\\Users\\admin\\OneDrive - NEOM\\Desktop\\imdb_sentiment\\.conda\\lib\\site-packages\\sentence_transformers\\models\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mAsym\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Asym\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mBoW\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BoW\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCLIPModel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CLIPModel\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCNN\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CNN\n",
      "File \u001b[1;32mc:\\Users\\admin\\OneDrive - NEOM\\Desktop\\imdb_sentiment\\.conda\\lib\\site-packages\\sentence_transformers\\models\\BoW.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor, nn\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WhitespaceTokenizer\n\u001b[0;32m     11\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBoW\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "File \u001b[1;32mc:\\Users\\admin\\OneDrive - NEOM\\Desktop\\imdb_sentiment\\.conda\\lib\\site-packages\\sentence_transformers\\models\\tokenizer\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mPhraseTokenizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PhraseTokenizer\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mWhitespaceTokenizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WhitespaceTokenizer\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mWordTokenizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ENGLISH_STOP_WORDS, WordTokenizer\n",
      "File \u001b[1;32mc:\\Users\\admin\\OneDrive - NEOM\\Desktop\\imdb_sentiment\\.conda\\lib\\site-packages\\sentence_transformers\\models\\tokenizer\\PhraseTokenizer.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstring\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Iterable, List\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NLTK_IMPORT_ERROR, is_nltk_available\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mWordTokenizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ENGLISH_STOP_WORDS, WordTokenizer\n\u001b[0;32m     12\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'NLTK_IMPORT_ERROR' from 'transformers.utils.import_utils' (c:\\Users\\admin\\OneDrive - NEOM\\Desktop\\imdb_sentiment\\.conda\\lib\\site-packages\\transformers\\utils\\import_utils.py)"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "# import transforms\n",
    "# from transforms.api import Input, Output, transform\n",
    "from bertopic import BERTopic\n",
    "import numpy as np\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "# from pyspark.sql.types import StructType, StructField, IntegerType, ArrayType, StringType, DoubleType, FloatType\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, PartOfSpeech\n",
    "import spacy\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "# import en_core_web_sm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# from transforms.external.systems import use_external_systems, EgressPolicy, Credential, ExportControl\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentimentGroup(compound):\n",
    "    if 0.66 <= compound < 1:\n",
    "        return 'Very Good'\n",
    "    elif 0.33 <= compound < 0.66:\n",
    "        return 'Good'\n",
    "    elif 0 <= compound < 0.33:\n",
    "        return 'Neutral Postive'\n",
    "    elif -0.33 <= compound < 0:\n",
    "        return 'Neutral Negative'\n",
    "    elif -0.66 <= compound < -0.33:\n",
    "        return 'Bad'\n",
    "    elif -1 <= compound < -0.66:\n",
    "        return 'Very Bad'\n",
    "       \n",
    " \n",
    "def getSentimentDict(sentence):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    score = analyzer.polarity_scores(sentence)\n",
    "    compound = score #score['compound']\n",
    "    return compound\n",
    "\n",
    "\n",
    "# params\n",
    "# params = params.dataframe()\n",
    "# params = params.collect()[0][:-1]\n",
    "# [n_neighbors, _, min_cluster_size, _, _] = params\n",
    "# n_neighbors = 15\n",
    "# min_cluster_size = 15\n",
    "\n",
    "n_neighbors=15\n",
    "min_topic_size=15\n",
    "min_cluster_size=15\n",
    "top_n_words=10\n",
    "diversity = 0.9\n",
    "\n",
    "# umap model\n",
    "umap_model = UMAP(n_neighbors=n_neighbors, n_components=5, min_dist=0, metric='cosine', random_state=42)\n",
    "\n",
    "# clustering\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\", min_df=2, ngram_range=(1, 2))\n",
    "# representation model\n",
    "keybert_model = KeyBERTInspired()\n",
    "mmr_model = MaximalMarginalRelevance(diversity)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\") #inshallah\n",
    "pos = PartOfSpeech(top_n_words=top_n_words)\n",
    "\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "# All representation models\n",
    "representation_model = {\n",
    "    \"KeyBERT\": keybert_model,\n",
    "    \"MMR\": mmr_model,\n",
    "    \"pos\": pos\n",
    "}\n",
    "\n",
    "#Train the model\n",
    "# model = BERTopic(\n",
    "#   # Pipeline models\n",
    "#   embedding_model = embedding_model,\n",
    "#   umap_model=umap_model,\n",
    "#   hdbscan_model=hdbscan_model,\n",
    "#   vectorizer_model=vectorizer_model,\n",
    "#   representation_model=representation_model,\n",
    "#   # Hyperparameters\n",
    "#   top_n_words = top_n_words,\n",
    "#   min_topic_size = min_topic_size,\n",
    "#   #nr_topics=75,\n",
    "#   ctfidf_model=ctfidf_model,\n",
    "#   verbose=True\n",
    "# )\n",
    "\n",
    "\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2',device=\"cpu\")\n",
    "\n",
    "model = BERTopic(\n",
    "    verbose=True,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    representation_model= representation_model,\n",
    "    embedding_model = embedding_model\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "source_df = source_df.dataframe()\n",
    "# source_df = source_df.limit(2000)\n",
    "df = source_df.toPandas()\n",
    "\n",
    "df = df.dropna(subset=['embedding'])\n",
    "\n",
    "docs = df.text.to_list()\n",
    "embeddings = np.array(df['embedding'].apply(lambda x: np.array(x).astype(np.float32)).to_list())\n",
    "topics, probs = model.fit_transform(docs, embeddings)\n",
    "df[\"topic\"] = topics\n",
    "df[\"probability\"] = probs\n",
    "\n",
    "# get sentiment\n",
    "df['sentiment_analysis'] = df['text'].apply(lambda x: getSentimentDict(x))\n",
    "df['sentiment_compound'] = df['sentiment_analysis'].apply(lambda x: x['compound'])\n",
    "df['sentiment'] = df['sentiment_compound'].apply(lambda x: getSentimentGroup(x)) # Good Bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from scipy.cluster import hierarchy as sch\n",
    "\n",
    "keybert_model = KeyBERTInspired()\n",
    "\n",
    "representation_model = {\n",
    "    \"KeyBERT\": keybert_model,\n",
    "    \"MMR\": mmr_model,\n",
    "    \"pos\": pos\n",
    "}\n",
    "\n",
    "model = BERTopic(\n",
    "    verbose=True,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    representation_model= representation_model,\n",
    "    embedding_model = embedding_model\n",
    ")\n",
    "\n",
    "hierarchical_topics = model.hierarchical_topics(docs)\n",
    "\n",
    "linkage_function = lambda x: sch.linkage(x, 'single', optimal_ordering=True)\n",
    "hierarchical_topics = model.hierarchical_topics(docs, linkage_function=linkage_function)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
