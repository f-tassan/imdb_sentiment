{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "# import transforms\n",
    "# from transforms.api import Input, Output, transform\n",
    "from bertopic import BERTopic\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "import ast\n",
    "# from pyspark.sql.types import StructType, StructField, IntegerType, ArrayType, StringType, DoubleType, FloatType\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, PartOfSpeech\n",
    "import spacy\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "# import en_core_web_sm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# from transforms.external.systems import use_external_systems, EgressPolicy, Credential, ExportControl\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentimentGroup(compound):\n",
    "    if 0.66 <= compound < 1:\n",
    "        return 'Very Good'\n",
    "    elif 0.33 <= compound < 0.66:\n",
    "        return 'Good'\n",
    "    elif 0 <= compound < 0.33:\n",
    "        return 'Neutral Postive'\n",
    "    elif -0.33 <= compound < 0:\n",
    "        return 'Neutral Negative'\n",
    "    elif -0.66 <= compound < -0.33:\n",
    "        return 'Bad'\n",
    "    elif -1 <= compound < -0.66:\n",
    "        return 'Very Bad'\n",
    "       \n",
    " \n",
    "def getSentimentDict(sentence):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    score = analyzer.polarity_scores(sentence)\n",
    "    compound = score #score['compound']\n",
    "    return compound\n",
    "\n",
    "\n",
    "# params\n",
    "# params = params.dataframe()\n",
    "# params = params.collect()[0][:-1]\n",
    "# [n_neighbors, _, min_cluster_size, _, _] = params\n",
    "# n_neighbors = 15\n",
    "# min_cluster_size = 15\n",
    "\n",
    "n_neighbors=15\n",
    "min_topic_size=15\n",
    "min_cluster_size=15\n",
    "top_n_words=10\n",
    "diversity = 0.9\n",
    "\n",
    "# umap model\n",
    "umap_model = UMAP(n_neighbors=n_neighbors, n_components=5, min_dist=0, metric='cosine', random_state=42)\n",
    "\n",
    "# clustering\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\", min_df=2, ngram_range=(1, 2))\n",
    "# representation model\n",
    "keybert_model = KeyBERTInspired()\n",
    "mmr_model = MaximalMarginalRelevance(diversity)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\") #inshallah\n",
    "pos = PartOfSpeech(top_n_words=top_n_words)\n",
    "\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "# All representation models\n",
    "representation_model = {\n",
    "    \"KeyBERT\": keybert_model,\n",
    "    \"MMR\": mmr_model,\n",
    "    \"pos\": pos\n",
    "}\n",
    "\n",
    "#Train the model\n",
    "# model = BERTopic(\n",
    "#   # Pipeline models\n",
    "#   embedding_model = embedding_model,\n",
    "#   umap_model=umap_model,\n",
    "#   hdbscan_model=hdbscan_model,\n",
    "#   vectorizer_model=vectorizer_model,\n",
    "#   representation_model=representation_model,\n",
    "#   # Hyperparameters\n",
    "#   top_n_words = top_n_words,\n",
    "#   min_topic_size = min_topic_size,\n",
    "#   #nr_topics=75,\n",
    "#   ctfidf_model=ctfidf_model,\n",
    "#   verbose=True\n",
    "# )\n",
    "\n",
    "\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2',device=\"cpu\")\n",
    "\n",
    "model = BERTopic(\n",
    "    verbose=True,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    representation_model= representation_model,\n",
    "    embedding_model = embedding_model\n",
    ")\n",
    "\n",
    "\n",
    "# source_df = source_df.dataframe()\n",
    "# # source_df = source_df.limit(2000)\n",
    "# df = source_df.toPandas()\n",
    "\n",
    "df = pd.read_csv('input/sentence_embedded.csv') # change the filename to your .csv file name and location\n",
    "\n",
    "df = df.dropna(subset=['embedding'])\n",
    "\n",
    "df['embedding'] = df['embedding'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "docs = df.text.to_list()\n",
    "embeddings = np.array(df['embedding'].apply(lambda x: np.array(x).astype(np.float32)).to_list())\n",
    "# embeddings = np.array(df['embedding'].apply(lambda x: np.array(x, dtype=np.float32)).to_list())\n",
    "topics, probs = model.fit_transform(docs, embeddings)\n",
    "df[\"topic\"] = topics\n",
    "df[\"probability\"] = probs\n",
    "\n",
    "# get sentiment\n",
    "df['sentiment_analysis'] = df['text'].apply(lambda x: getSentimentDict(x))\n",
    "df['sentiment_compound'] = df['sentiment_analysis'].apply(lambda x: x['compound'])\n",
    "df['sentiment'] = df['sentiment_compound'].apply(lambda x: getSentimentGroup(x)) # Good Bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy as sch\n",
    "\n",
    "hierarchical_topics = model.hierarchical_topics(docs)\n",
    "\n",
    "linkage_function = lambda x: sch.linkage(x, 'single', optimal_ordering=True)\n",
    "hierarchical_topics = model.hierarchical_topics(docs, linkage_function=linkage_function)\n",
    "\n",
    "# visulize the topics hierarchy\n",
    "model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hier_topics = model.hierarchical_topics(docs)\n",
    "\n",
    "hier_topics.to_csv(\"temp_files/hier_topics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hier_topics = model.hierarchical_topics(docs)\n",
    "\n",
    "expanded_rows = []\n",
    "\n",
    "# Iterate through the DataFrame and expand the arrays\n",
    "for index, row in hier_topics.iterrows():\n",
    "    # array_values = row['Topics'].strip('[]').split(',')\n",
    "    for value in row['Topics']:\n",
    "        expanded_row = row.copy()\n",
    "        expanded_row['Topics'] = value\n",
    "        expanded_rows.append(expanded_row)\n",
    "\n",
    "# Convert the list of rows to a DataFrame\n",
    "expanded_df = pd.DataFrame(expanded_rows)\n",
    "\n",
    "# Save the expanded DataFrame to a new CSV file\n",
    "expanded_df.to_csv('temp_files/expanded_output.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hier_topics = model.hierarchical_topics(docs)\n",
    "\n",
    "hier_topics = hier_topics.explode('Topics')\n",
    "\n",
    "# Save the expanded DataFrame to a new CSV file\n",
    "hier_topics.to_csv('temp_files/expanded_output.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = model.get_topic_tree(hierarchical_topics)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_topics(file_path, output_path):\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    child_ids = set(df['Child_Left_ID']).union(set(df['Child_Right_ID']))\n",
    "    parent_ids = set(df['Parent_ID'])\n",
    "    \n",
    "    criteria = df.apply(lambda row: row['Child_Left_ID'] not in parent_ids and \n",
    "                                   row['Child_Right_ID'] not in parent_ids, axis=1)\n",
    "    \n",
    "    df['category'] = criteria.apply(lambda x: 'yes' if x else 'no')\n",
    "    \n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"File saved to: {output_path}\")\n",
    "\n",
    "# Replace with your file paths\n",
    "categorize_topics(\"temp_files/hier_topics.csv\", \"temp_files/hier_topics_with_categories.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_topics_with_levels(file_path, output_path):\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    child_ids = set(df['Child_Left_ID']).union(set(df['Child_Right_ID']))\n",
    "    parent_ids = set(df['Parent_ID'])\n",
    "    \n",
    "    df['category'] = df.apply(lambda row: row['Child_Left_ID'] not in parent_ids and \n",
    "                                           row['Child_Right_ID'] not in parent_ids, axis=1)\n",
    "    df['category'] = df['category'].apply(lambda x: 'yes' if x else 'no')\n",
    "    \n",
    "    df['category_lvl2'] = 'no'\n",
    "    df['category_lvl3'] = 'no'\n",
    "    \n",
    "    category_rows = df[df['category'] == 'yes']\n",
    "    for index, row in category_rows.iterrows():\n",
    "        parent_id = row['Parent_ID']\n",
    "        matching_rows = df[(df['Child_Left_ID'] == parent_id) | (df['Child_Right_ID'] == parent_id)]\n",
    "        df.loc[matching_rows.index, 'category_lvl2'] = 'yes'\n",
    "    \n",
    "    category_lvl2_rows = df[df['category_lvl2'] == 'yes']\n",
    "    for index, row in category_lvl2_rows.iterrows():\n",
    "        parent_id = row['Parent_ID']\n",
    "        matching_rows = df[(df['Child_Left_ID'] == parent_id) | (df['Child_Right_ID'] == parent_id)]\n",
    "        df.loc[matching_rows.index, 'category_lvl3'] = 'yes'\n",
    "    \n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"File saved to: {output_path}\")\n",
    "\n",
    "# Replace with your file paths\n",
    "categorize_topics_with_levels('temp_files/hier_topics.csv', 'temp_files/hier_topics_with_categories3.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
